---
title: "从v2.13升级至v3.2"
weight: 4
description: 介绍var_oem_name产品如何进行升级操作。
---

## 升级介绍

本文档介绍如何从{{<oem_name>}} 2.13.x版本升级到3.2.x版本。

{{<oem_name>}} 3.x版本不同于2.x版本采用传统部署方法运行CentOS系统上，而是基于Kubernetes Operator技术部署运行在Kubernetes上，并将平台服务组件等容器化部署运行在Kubernetes集群。

## 升级前准备

1. 本次升级为停服升级，即控制节点和计算节点都将关闭并禁用所有服务。请确保在不影响业务的时间进行升级操作。
2. 升级前，请自行备份数据库。

## 升级步骤

本次升级以最简组网（1控制节点+1计算节点）为例。

### 关闭系统所有服务

1. 分别在控制节点、计算节点上创建禁用{{<oem_name>}}的脚本文件。

    ```bash
    # 创建禁用服务的脚本
    $ echo '#!/bin/bash
     
    pushd $(dirname $(readlink -f "$BASH_SOURCE")) > /dev/null
    CUR_DIR=$(pwd)
    ROOT_DIR=$(cd .. && pwd)
    popd > /dev/null
    #################################################################
     
    SCRIPTS=$ROOT_DIR/scripts
     
    . $ROOT_DIR/vars
    . $CUR_DIR/functions
    . $CUR_DIR/all_services.sh
     
    action=disable
     
    for i in "${COMPONENTS[@]}"
    do
            info "Component $i:"
            comp_group=services_${i,,}[@]
            warn start disable ${!comp_group}
            services_action $action ${!comp_group}
    done
    ' > /opt/yunionsetup/scripts/all_disable.sh
    # 添加执行脚本的权限
    $ chmod +x /opt/yunionsetup/scripts/all_disable.sh
    ```

2. 分别在控制节点、计算节点上停止并禁用所有服务。

    ```bash
    # 停止所有服务
    $ /opt/yunionsetup/scripts/all_stop.sh
    # 禁用所有服务
    $ /opt/yunionsetup/scripts/all_disable.sh
    ```

### 安装3.x版本相关依赖

以下操作分别在控制节点和计算节点上执行。

#### 安装配置docker

1. 安装docker，docker版本要求为18.09或19.03版本。
    
    ```bash
    $ yum install -y yum-utils bash-completion
    # 添加3.2版本的rpm源
    $ yum-config-manager --add-repo https://iso.yunion.cn/yumrepo-3.2/yunion.repo
    $ yum install -y docker-ce-19.03.9 docker-ce-cli-19.03.9 containerd.io
    ```

2. 配置docker。

    ```bash
    $ mkdir -p /etc/docker
    $ cat <<EOF >/etc/docker/daemon.json
    {
      "bridge": "none",
      "iptables": false,
      "exec-opts":
        [
          "native.cgroupdriver=systemd"
        ],
      "data-root": "/opt/docker",
      "live-restore": true,
      "log-driver": "json-file",
      "log-opts":
        {
          "max-size": "100m"
        },
      "registry-images":
        [
          "https://lje6zxpk.image.aliyuncs.com",
          "https://lms7sxqp.image.aliyuncs.com",
          "https://registry.docker-cn.com"
        ]
    }
    EOF
    ``` 

3. 启动docker。
    
    ```bash
    $ systemctl enable --now docker
    ```  

#### 安装配置kubelet

1. 从{{<oem_name>}} rpm的yum源安装Kubernetes 1.15.8，并设置kubelet开机自启动。

    ```bash
    $ yum install -y bridge-utils ipvsadm conntrack-tools \
        jq kubelet-1.15.8-0 kubectl-1.15.8-0 kubeadm-1.15.8-0
    $ echo 'source <(kubectl completion bash)' >> ~/.bashrc && source ~/.bashrc
    $ source /etc/profile
    $ systemctl enable kubelet
    ```

2. 安装完kubernetes相关的二进制后，还需要对系统做一些配置并启ipvs作为kube-proxy内部的service负载均衡。

    ```bash
    # 禁用 swap
    $ swapoff -a
    # 如果设置了自动挂载 swap，需要去 /etc/fstab 里面注释掉挂载 swap 那一行

    # 关闭 selinux
    $ setenforce  0
    $ sed -i 's/SELINUX=enforcing/SELINUX=disabled/' /etc/selinux/config

    # 禁用 firewalld
    $ systemctl stop firewalld
    $ systemctl disable firewalld

    # 禁用 NetworkManager
    $ systemctl stop NetworkManager
    $ systemctl disable NetworkManager

    # 做一些 sysctl 的配置, kubernetes 要求
    $ modprobe br_netfilter

    $ cat <<EOF >> /etc/sysctl.conf
    net.bridge.bridge-nf-call-iptables=1
    net.bridge.bridge-nf-call-ip6tables=1
    net.ipv4.ip_forward=1
    EOF

    $ sysctl -p

    # 配置并开启 ipvs
    $ cat <<EOF > /etc/sysconfig/modules/ipvs.modules
    #!/bin/bash
    ipvs_modules="ip_vs ip_vs_lc ip_vs_wlc ip_vs_rr ip_vs_wrr ip_vs_lblc ip_vs_lblcr ip_vs_dh ip_vs_sh ip_vs_fo ip_vs_nq ip_vs_sed ip_vs_ftp nf_conntrack_ipv4"
    for kernel_module in \${ipvs_modules}; do
        /sbin/modinfo -F filename \${kernel_module} > /dev/null 2>&1
        if [ $? -eq 0 ]; then
            /sbin/modprobe \${kernel_module}
        fi
    done
    EOF

    $ chmod 755 /etc/sysconfig/modules/ipvs.modules && bash /etc/sysconfig/modules/ipvs.modules && lsmod | grep ip_vs
    ```

#### 安装部署ocadm部署工具

**控制节点**

控制节点需要安装climc命令行工具和ocadm部署工具。

```bash
# 安装climc云平台命令行工具和ocadm部署工具
$ yum install -y yunion-climc yunion-ocadm
# climc 在 /opt/yunion/bin 目录下，根据自己的需要加到 bash 或者 zsh 配置文件里面
$ echo 'export PATH=$PATH:/opt/yunion/bin' >> ~/.bashrc && source ~/.bashrc
```

**计算节点**

计算节点仅需要按照ocadm工具即可。

```bash
$ yum install -y yunion-ocadm
$ echo 'export PATH=$PATH:/opt/yunion/bin' >> ~/.bashrc && source ~/.bashrc
```

#### 安装yunion-executor-server

```bash
# 安装yunion-executor-server
$ yum install -y yunion-executor-server
# 启用yunion-executor服务
$ systemctl enable --now yunion-executor
```

### 部署Kubernetes集群

**控制节点**

1. 在控制节点上执行`ocadm init`初始化集群。

    ```bash
    # 由于我们将从配置文件中读取mysql的配置，所以就不用配置和检查mysql了
    # 如果当前节点是glance、esxi-agent、baremetal-agent服务所在的节点，则需要加上--glance-node、--baremetal-node、--esxi-node的参数，用来标识glance、baremetal agent、esxi agent调度到这个node上
    $ ocadm init --enable-host-agent --glance-node --baremetal-node --esxi-node --ignore-preflight-errors Mysql
    
    # 命令执行完成后，将会生成下面命令供其他计算节点加入Kubernetes集群，将命令拷贝到计算节点上执行
    $ ocadm join 10.127.90.210:6443 --token trddqw.5eh5s63yn3td8pgz \
          --discovery-token-ca-cert-hash sha256:544e335cb85344952de2050cb13a84d154430a4c6f854916df0e8db60b1bc0c0
    ```

2. 部署完成后执行以下命令，使kubernetes配置生效。

    ```bash
    $ mkdir -p $HOME/.kube
    $ sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
    $ sudo chown $(id -u):$(id -g) $HOME/.kube/config
    ```

**计算节点**

  
1. 在计算节点执行以下命令加入集群。

    ```bash
    # 计算节点如启用了glance、baremetal agent、esxi服务时也需要在加入集群时添加--glance-node、--baremetal-node、--esxi-node的参数
    $ ocadm join $api_server_addr \
        --enable-host-agent \
        --token $token \
        --discovery-token-unsafe-skip-ca-verification \
        --glance-node --baremetal-node --esxi-node  
    ```

### 创建集群

当kubernetes集群部署完成后，就可以通过`ocadm cluster create`创建{{<oem_name>}}集群.

```bash
# --use-ee为商业版，
$ ocadm cluster create --upgrade-from-v2 --use-ee --wait
```

### 迁移数据

{{<oem_name>}} 3.x版本baremetal数据存放到在PVC中，所以需要手动将之前的数据拷贝到PVC下。

```bash
# 等待baremetal agent的pod创建完成后获取pv路径
$ kubectl get pv | grep baremetal | awk '{ print $1 }' | xargs kubectl get pv -o yaml | grep -w 'path:'
    path: /opt/local-path-provisioner/pvc-076981f2-bee8-4db7-90c3-3f025e091063
```
将“/opt/cloud/workspace/”目录下的baremetal、bm_image_cache、bm_boot_iso目录下的文件拷贝到上一步骤获取的path“/opt/local-path-provisioner/pvc-076981f2-bee8-4db7-90c3-3f025e091063”路径下。

```bash
# 拷贝文件
$ cp -r /opt/cloud/workspace/baremetal/* /opt/local-path-provisioner/pvc-076981f2-bee8-4db7-90c3-3f025e091063/baremetal/
$ cp -r /opt/cloud/workspace/bm_image_cache/* /opt/local-path-provisioner/pvc-076981f2-bee8-4db7-90c3-3f025e091063/bm_image_cache/
$ cp -r /opt/cloud/workspace/bm_boot_iso/* /opt/local-path-provisioner/pvc-076981f2-bee8-4db7-90c3-3f025e091063/bm_boot_iso/
```

### 删除多余服务

{{<oem_name>}}升级到3.x版本后，有一些服务（如kapacitor、meteralert）已经下线了，需要清除对应服务的信息。

```bash
# 初始化连接集群的管理员认证信息
$ source <(ocadm cluster rcadmin)

# 清除kapacitor服务信息
# 获取kapacitor endpoint的ID
$ climc endpoint-list --detail --search kapacitor
# 分别清除内部接口、公共接口的kapacitor的endpoint信息
$ climc endpoint-update --disabled <ID>
$ climc endpoint-delete <ID>
# 清除kapacitor的service信息
$ climc service-update --disabled kapacitor
$ climc service-delete kapacitor

# 清除meteralert服务信息
# 获取meteralert endpoint的ID，包括内部和公共的endpoint信息
$ climc endpoint-list --detail --search meteralert
# 分别清除内部接口、公共接口的meteralert的endpoint信息
$ climc endpoint-update --disabled <ID>
$ climc endpoint-delete <ID>
# 清除meteralert的service信息
$ climc service-update --disabled meteralert
$ climc service-delete meteralert
```


